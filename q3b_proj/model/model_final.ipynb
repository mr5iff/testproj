{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use polyglot for tokenizing and word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyglot\n",
    "from polyglot.text import Text, Word\n",
    "from polyglot.mapping import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sklearn for the utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use keras with tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense, LSTM, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use hyperas for hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    hyperas has some weird error if the first_n_records or TextClassificationDataSet is parametrized... should refactor\n",
    "    \"\"\"\n",
    "    MAX_WORD_COUNT = 150\n",
    "    class TextClassificationDataSet(object):\n",
    "        def __init__(self, \n",
    "                     file_path,\n",
    "                     word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
    "                     MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
    "                     text_col_name='text',\n",
    "                     label_col_name='tags',\n",
    "                     one_hot_encoder=None):\n",
    "            self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
    "\n",
    "            self.df = pd.read_csv(file_path)\n",
    "            self.text_col_name = text_col_name\n",
    "            self.label_col_name = label_col_name\n",
    "\n",
    "            if label_col_name is not None:\n",
    "                self.label_encoder = self._fit_label_encoder(label_col_name)\n",
    "                self.onehot_encoder = self._fit_onehot_encoder()\n",
    "\n",
    "            if one_hot_encoder is not None:\n",
    "                self.one_hot_encoder = one_hot_encoder\n",
    "\n",
    "            self.embeddings = self._load_word_embeddings(word_embedding)\n",
    "\n",
    "            self.features = None\n",
    "            self.labels = None\n",
    "\n",
    "        def get_features(self, use_cache=True):        \n",
    "            if use_cache and self.features is not None:\n",
    "                return self.features\n",
    "            clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
    "            self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
    "            return self.features\n",
    "\n",
    "        def get_labels(self, use_cache=True):\n",
    "            if self.label_col_name is None:\n",
    "                raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
    "            if use_cache and self.labels is not None:\n",
    "                return self.labels\n",
    "            self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
    "            return self.labels\n",
    "\n",
    "        def _parse_text(self, text):\n",
    "            if isinstance(text, str):\n",
    "                text_parsed = Text(text)\n",
    "            else:\n",
    "                text_parsed = text\n",
    "            return text_parsed\n",
    "\n",
    "        def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
    "            if isinstance(article_text, str):\n",
    "                article_parsed = self._parse_text(article_text)\n",
    "\n",
    "            sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
    "            return sentences_words_embedding\n",
    "\n",
    "        def _load_word_embeddings(self, word_embedding):\n",
    "            if isinstance(word_embedding, Embedding):\n",
    "                return word_embedding\n",
    "            else:\n",
    "                return Embedding.load(word_embedding)\n",
    "\n",
    "        def _load_data_from_csv(self, file_path):\n",
    "            return pd.read_csv(file_path)\n",
    "\n",
    "        def _get_clean_text_col(self, text_col):\n",
    "            \"\"\"remove html tags in text\"\"\"\n",
    "            text_col = self.df[text_col]\n",
    "            return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
    "\n",
    "        def _fit_label_encoder(self, label_col):\n",
    "            label_encoder = preprocessing.LabelEncoder()\n",
    "            label_encoder.fit(self.df[label_col].tolist())\n",
    "            self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
    "            self.label_encoder = label_encoder\n",
    "            return label_encoder\n",
    "\n",
    "        def _fit_onehot_encoder(self):\n",
    "            onehot_encoder = preprocessing.OneHotEncoder()\n",
    "            onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
    "            self.onehot_encoder = onehot_encoder\n",
    "            return onehot_encoder\n",
    "    dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(dataset_train.get_features(), dataset_train.get_labels(), test_size=0.2, random_state=42)\n",
    "    return X_train, y_train, X_validate, y_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_first1024():\n",
    "    \"\"\" \n",
    "    same as data(), just that it only returns the first 1024 rows of the input \n",
    "    => faster for searching hyper params\n",
    "    \n",
    "    hyperas has some weird error if the first_n_records or TextClassificationDataSet is parametrized... should refactor\n",
    "    \"\"\"\n",
    "    MAX_WORD_COUNT = 150\n",
    "    first_n_records = 1024\n",
    "    class TextClassificationDataSet(object):\n",
    "        def __init__(self, \n",
    "                     file_path,\n",
    "                     word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
    "                     MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
    "                     text_col_name='text',\n",
    "                     label_col_name='tags',\n",
    "                     one_hot_encoder=None):\n",
    "            self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
    "\n",
    "            self.df = pd.read_csv(file_path).head(first_n_records)\n",
    "            self.text_col_name = text_col_name\n",
    "            self.label_col_name = label_col_name\n",
    "\n",
    "            if label_col_name is not None:\n",
    "                self.label_encoder = self._fit_label_encoder(label_col_name)\n",
    "                self.onehot_encoder = self._fit_onehot_encoder()\n",
    "\n",
    "            if one_hot_encoder is not None:\n",
    "                self.one_hot_encoder = one_hot_encoder\n",
    "\n",
    "            self.embeddings = self._load_word_embeddings(word_embedding)\n",
    "\n",
    "            self.features = None\n",
    "            self.labels = None\n",
    "\n",
    "        def get_features(self, use_cache=True):\n",
    "            if use_cache and self.features is not None:\n",
    "                return self.features\n",
    "            clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
    "            self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
    "            return self.features\n",
    "\n",
    "        def get_labels(self, use_cache=True):\n",
    "            if self.label_col_name is None:\n",
    "                raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
    "            if use_cache and self.labels is not None:\n",
    "                return self.labels\n",
    "            self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
    "            return self.labels\n",
    "\n",
    "        def _parse_text(self, text):\n",
    "            if isinstance(text, str):\n",
    "                text_parsed = Text(text)\n",
    "            else:\n",
    "                text_parsed = text\n",
    "            return text_parsed\n",
    "\n",
    "        def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
    "            if isinstance(article_text, str):\n",
    "                article_parsed = self._parse_text(article_text)\n",
    "\n",
    "            sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
    "            return sentences_words_embedding\n",
    "\n",
    "        def _load_word_embeddings(self, word_embedding):\n",
    "            if isinstance(word_embedding, Embedding):\n",
    "                return word_embedding\n",
    "            else:\n",
    "                return Embedding.load(word_embedding)\n",
    "\n",
    "        def _load_data_from_csv(self, file_path):\n",
    "            return pd.read_csv(file_path)\n",
    "\n",
    "        def _get_clean_text_col(self, text_col):\n",
    "            \"\"\"remove html tags in text\"\"\"\n",
    "            text_col = self.df[text_col]\n",
    "            return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
    "\n",
    "        def _fit_label_encoder(self, label_col):\n",
    "            label_encoder = preprocessing.LabelEncoder()\n",
    "            label_encoder.fit(self.df[label_col].tolist())\n",
    "            self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
    "            self.label_encoder = label_encoder\n",
    "            return label_encoder\n",
    "\n",
    "        def _fit_onehot_encoder(self):\n",
    "            onehot_encoder = preprocessing.OneHotEncoder()\n",
    "            onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
    "            self.onehot_encoder = onehot_encoder\n",
    "            return onehot_encoder\n",
    "    dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(dataset_train.get_features(), dataset_train.get_labels(), test_size=0.2, random_state=42)\n",
    "    return X_train, y_train, X_validate, y_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_model_seq(X_train, y_train, X_validate, y_validate):\n",
    "    \"\"\"\n",
    "    Defines the computational graph.\n",
    "    \"\"\"\n",
    "    MAX_WORD_COUNT = 150\n",
    "    embedding_size = 64\n",
    "    tag_classes_count = 3\n",
    "    \n",
    "    batch_size = {{choice([128, 256, 512])}}\n",
    "    lstm_units = {{choice([64, 128, 256, 512])}}\n",
    "    dense_units = {{choice([64, 128, 256, 512])}}\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(lstm_units, input_shape=(MAX_WORD_COUNT, embedding_size), name='LSTM'))\n",
    "    \n",
    "    model.add(Dense(dense_units, activation='relu', name='Dense_1'))\n",
    "    model.add(Dense(dense_units, activation='relu', name='Dense_2'))\n",
    "    model.add(Dense(dense_units, activation='relu', name='Dense_3'))\n",
    "\n",
    "    model.add(Dense(tag_classes_count, activation='softmax', name='main_output'))\n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam', 'adagrad', 'nadam', 'adadelta'])}}, \n",
    "              loss={'main_output': 'categorical_crossentropy'}, \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              validation_data=(X_validate, y_validate))\n",
    "    \n",
    "    score, acc = model.evaluate(X_validate, y_validate, batch_size=batch_size, verbose=0)\n",
    "    print('Test Accuracy:{}'.format(acc))\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_validate, y_validate = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from bs4 import BeautifulSoup\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import polyglot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from polyglot.text import Text, Word\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from polyglot.mapping import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, LSTM, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model, Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import optimizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe, rand\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'batch_size': hp.choice('batch_size', [128, 256, 512]),\n",
      "        'lstm_units': hp.choice('lstm_units', [64, 128, 256, 512]),\n",
      "        'lstm_units_1': hp.choice('lstm_units_1', [64, 128, 256, 512]),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'adagrad', 'nadam', 'adadelta']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \"\"\" \n",
      "   3: same as data(), just that it only returns the first 1024 rows of the input \n",
      "   4: => faster for searching hyper params\n",
      "   5: \n",
      "   6: hyperas has some weird error if the first_n_records or TextClassificationDataSet is parametrized... should refactor\n",
      "   7: \"\"\"\n",
      "   8: MAX_WORD_COUNT = 150\n",
      "   9: first_n_records = 1024\n",
      "  10: class TextClassificationDataSet(object):\n",
      "  11:     def __init__(self, \n",
      "  12:                  file_path,\n",
      "  13:                  word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
      "  14:                  MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
      "  15:                  text_col_name='text',\n",
      "  16:                  label_col_name='tags',\n",
      "  17:                  one_hot_encoder=None):\n",
      "  18:         self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
      "  19: \n",
      "  20:         self.df = pd.read_csv(file_path).head(first_n_records)\n",
      "  21:         self.text_col_name = text_col_name\n",
      "  22:         self.label_col_name = label_col_name\n",
      "  23: \n",
      "  24:         if label_col_name is not None:\n",
      "  25:             self.label_encoder = self._fit_label_encoder(label_col_name)\n",
      "  26:             self.onehot_encoder = self._fit_onehot_encoder()\n",
      "  27: \n",
      "  28:         if one_hot_encoder is not None:\n",
      "  29:             self.one_hot_encoder = one_hot_encoder\n",
      "  30: \n",
      "  31:         self.embeddings = self._load_word_embeddings(word_embedding)\n",
      "  32: \n",
      "  33:         self.features = None\n",
      "  34:         self.labels = None\n",
      "  35: \n",
      "  36:     def get_features(self, use_cache=True):\n",
      "  37:         if use_cache and self.features is not None:\n",
      "  38:             return self.features\n",
      "  39:         clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
      "  40:         self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
      "  41:         return self.features\n",
      "  42: \n",
      "  43:     def get_labels(self, use_cache=True):\n",
      "  44:         if self.label_col_name is None:\n",
      "  45:             raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
      "  46:         if use_cache and self.labels is not None:\n",
      "  47:             return self.labels\n",
      "  48:         self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
      "  49:         return self.labels\n",
      "  50: \n",
      "  51:     def _parse_text(self, text):\n",
      "  52:         if isinstance(text, str):\n",
      "  53:             text_parsed = Text(text)\n",
      "  54:         else:\n",
      "  55:             text_parsed = text\n",
      "  56:         return text_parsed\n",
      "  57: \n",
      "  58:     def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
      "  59:         if isinstance(article_text, str):\n",
      "  60:             article_parsed = self._parse_text(article_text)\n",
      "  61: \n",
      "  62:         sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
      "  63:         return sentences_words_embedding\n",
      "  64: \n",
      "  65:     def _load_word_embeddings(self, word_embedding):\n",
      "  66:         if isinstance(word_embedding, Embedding):\n",
      "  67:             return word_embedding\n",
      "  68:         else:\n",
      "  69:             return Embedding.load(word_embedding)\n",
      "  70: \n",
      "  71:     def _load_data_from_csv(self, file_path):\n",
      "  72:         return pd.read_csv(file_path)\n",
      "  73: \n",
      "  74:     def _get_clean_text_col(self, text_col):\n",
      "  75:         \"\"\"remove html tags in text\"\"\"\n",
      "  76:         text_col = self.df[text_col]\n",
      "  77:         return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
      "  78: \n",
      "  79:     def _fit_label_encoder(self, label_col):\n",
      "  80:         label_encoder = preprocessing.LabelEncoder()\n",
      "  81:         label_encoder.fit(self.df[label_col].tolist())\n",
      "  82:         self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
      "  83:         self.label_encoder = label_encoder\n",
      "  84:         return label_encoder\n",
      "  85: \n",
      "  86:     def _fit_onehot_encoder(self):\n",
      "  87:         onehot_encoder = preprocessing.OneHotEncoder()\n",
      "  88:         onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
      "  89:         self.onehot_encoder = onehot_encoder\n",
      "  90:         return onehot_encoder\n",
      "  91: dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')\n",
      "  92: X_train, X_validate, y_train, y_validate = train_test_split(dataset_train.get_features(), dataset_train.get_labels(), test_size=0.2, random_state=42)\n",
      "  93: \n",
      "  94: \n",
      "  95: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Defines the computational graph.\n",
      "   5:     \"\"\"\n",
      "   6:     MAX_WORD_COUNT = 150\n",
      "   7:     embedding_size = 64\n",
      "   8:     tag_classes_count = 3\n",
      "   9:     \n",
      "  10:     batch_size = space['batch_size']\n",
      "  11:     lstm_units = space['lstm_units']\n",
      "  12:     dense_units = space['lstm_units_1']\n",
      "  13:     \n",
      "  14:     model = Sequential()\n",
      "  15: \n",
      "  16:     model.add(LSTM(lstm_units, input_shape=(MAX_WORD_COUNT, embedding_size), name='LSTM'))\n",
      "  17:     \n",
      "  18:     model.add(Dense(dense_units, activation='relu', name='Dense_1'))\n",
      "  19:     model.add(Dense(dense_units, activation='relu', name='Dense_2'))\n",
      "  20:     model.add(Dense(dense_units, activation='relu', name='Dense_3'))\n",
      "  21: \n",
      "  22:     model.add(Dense(tag_classes_count, activation='softmax', name='main_output'))\n",
      "  23:     model.compile(optimizer=space['optimizer'], \n",
      "  24:               loss={'main_output': 'categorical_crossentropy'}, \n",
      "  25:               metrics=['accuracy'])\n",
      "  26:     \n",
      "  27:     model.fit(X_train, y_train,\n",
      "  28:               batch_size=batch_size,\n",
      "  29:               epochs=1,\n",
      "  30:               validation_data=(X_validate, y_validate))\n",
      "  31:     \n",
      "  32:     score, acc = model.evaluate(X_validate, y_validate, batch_size=batch_size, verbose=0)\n",
      "  33:     print('Test Accuracy:{}'.format(acc))\n",
      "  34:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  35: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 16s - loss: 1.0613 - acc: 0.5128 - val_loss: 0.9584 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0865 - acc: 0.4396 - val_loss: 1.0125 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 4s - loss: 1.7196 - acc: 0.4921 - val_loss: 1.0549 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 42s - loss: 1.0126 - acc: 0.4969 - val_loss: 0.8924 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 42s - loss: 1.0726 - acc: 0.3871 - val_loss: 1.5691 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 15s - loss: 3.5338 - acc: 0.3541 - val_loss: 6.9976 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 5s - loss: 0.9810 - acc: 0.5165 - val_loss: 0.8653 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 44s - loss: 5.5780 - acc: 0.4591 - val_loss: 6.9976 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 39s - loss: 1.0948 - acc: 0.3480 - val_loss: 1.0441 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 41s - loss: 1.1116 - acc: 0.4737 - val_loss: 1.0387 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 40s - loss: 1.0557 - acc: 0.4957 - val_loss: 1.0577 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 4s - loss: 1.0886 - acc: 0.4322 - val_loss: 1.0591 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 41s - loss: 1.0690 - acc: 0.3443 - val_loss: 2.2899 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 6s - loss: 1.0458 - acc: 0.4469 - val_loss: 0.9821 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 5s - loss: 1.0386 - acc: 0.4933 - val_loss: 0.9717 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 5s - loss: 1.0833 - acc: 0.4310 - val_loss: 1.0147 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0440 - acc: 0.5128 - val_loss: 0.9504 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 40s - loss: 1.1034 - acc: 0.4188 - val_loss: 1.0460 - val_acc: 0.5415\n",
      "Test Accuracy:0.5414634172509356\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 4s - loss: 1.0907 - acc: 0.4261 - val_loss: 1.0575 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 5s - loss: 1.0297 - acc: 0.5226 - val_loss: 0.9633 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0336 - acc: 0.5519 - val_loss: 0.9579 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0680 - acc: 0.4554 - val_loss: 0.9858 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0530 - acc: 0.5031 - val_loss: 0.9668 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0707 - acc: 0.4811 - val_loss: 0.9996 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0495 - acc: 0.5140 - val_loss: 0.9704 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 9s - loss: 1.0324 - acc: 0.5531 - val_loss: 0.9651 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0337 - acc: 0.5531 - val_loss: 0.9594 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0577 - acc: 0.5165 - val_loss: 0.9909 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 19s - loss: 1.1065 - acc: 0.5140 - val_loss: 0.8791 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.0091 - acc: 0.5507 - val_loss: 0.9263 - val_acc: 0.6244\n",
      "Test Accuracy:0.6243902328537732\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.0564 - acc: 0.4957 - val_loss: 0.9628 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 18s - loss: 1.0916 - acc: 0.4860 - val_loss: 0.9580 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.0457 - acc: 0.5018 - val_loss: 0.9305 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 18s - loss: 1.1526 - acc: 0.5226 - val_loss: 0.9271 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 18s - loss: 1.0091 - acc: 0.4957 - val_loss: 0.8861 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 18s - loss: 1.1385 - acc: 0.5055 - val_loss: 0.9386 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.0986 - acc: 0.5421 - val_loss: 0.9551 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.3883 - acc: 0.4957 - val_loss: 0.9116 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 6s - loss: 1.0048 - acc: 0.5531 - val_loss: 0.9466 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0736 - acc: 0.5507 - val_loss: 1.0055 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819/819 [==============================] - 6s - loss: 1.0226 - acc: 0.5495 - val_loss: 0.9821 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 9s - loss: 2.5498 - acc: 0.5409 - val_loss: 1.0489 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 9s - loss: 1.0165 - acc: 0.5519 - val_loss: 0.9368 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.2276 - acc: 0.4481 - val_loss: 0.9981 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 44s - loss: 1.0802 - acc: 0.5531 - val_loss: 1.0413 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 11s - loss: 6.3304 - acc: 0.5250 - val_loss: 6.9976 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.2694 - acc: 0.4957 - val_loss: 0.9899 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 43s - loss: 1.2028 - acc: 0.5275 - val_loss: 1.0174 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0977 - acc: 0.3700 - val_loss: 1.0523 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.3056 - acc: 0.4420 - val_loss: 0.9941 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 45s - loss: 1.0520 - acc: 0.5067 - val_loss: 0.9669 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.5201 - acc: 0.4017 - val_loss: 1.0245 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 19s - loss: 1.0543 - acc: 0.3553 - val_loss: 1.6306 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0228 - acc: 0.5531 - val_loss: 0.9628 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 44s - loss: 1.9597 - acc: 0.4505 - val_loss: 1.0157 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0535 - acc: 0.5250 - val_loss: 0.9788 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.0031 - acc: 0.5287 - val_loss: 0.9299 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 19s - loss: 1.1240 - acc: 0.5372 - val_loss: 1.0775 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 11s - loss: 1.0428 - acc: 0.5275 - val_loss: 0.9531 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 7s - loss: 1.0390 - acc: 0.4872 - val_loss: 0.9280 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 45s - loss: 1.0444 - acc: 0.4969 - val_loss: 0.9633 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 19s - loss: 1.0460 - acc: 0.5458 - val_loss: 0.9754 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 21s - loss: 1.0775 - acc: 0.3602 - val_loss: 0.9774 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.0294 - acc: 0.4872 - val_loss: 0.9965 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 21s - loss: 1.1758 - acc: 0.5495 - val_loss: 0.9937 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.0211 - acc: 0.4786 - val_loss: 1.0020 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 141s - loss: 1.0362 - acc: 0.5226 - val_loss: 0.9478 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 32s - loss: 1.0138 - acc: 0.4957 - val_loss: 0.8979 - val_acc: 0.5707\n",
      "Test Accuracy:0.5707317099338625\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.1184 - acc: 0.5372 - val_loss: 0.9242 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 1696s - loss: 1.0838 - acc: 0.4212 - val_loss: 0.9571 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n"
     ]
    }
   ],
   "source": [
    "# import gc; gc.collect()\n",
    "trials = Trials()\n",
    "best_run, best_model, space = optim.minimize(model=search_model_seq,\n",
    "                                      data=data_first1024,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=70,\n",
    "                                      trials=trials,\n",
    "                                      notebook_name='model_final',\n",
    "                                             eval_space=True,   # <-- this is the line that puts real values into 'best_run'\n",
    "                                             return_space=True  # <-- this allows you to save the space for later evaluations \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "779/779 [==============================] - 6s     \n",
      "[0.94468809658510844, 0.59563543026805388]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'batch_size': 128, 'lstm_units': 256, 'lstm_units_1': 256, 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_validate, y_validate))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'lstm_units': 256,\n",
       " 'lstm_units_1': 256,\n",
       " 'optimizer': 'rmsprop'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train,\n",
    "              batch_size=best_run['batch_size'],\n",
    "              epochs=20,\n",
    "#               verbose=2,\n",
    "              validation_data=(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 [==============================] - 5s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27436906497193853, 0.91014120659871034]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.models' from '/Users/ericng/Workspace/hk01_test/q3b_proj/model/tag-clf/lib/python3.6/site-packages/keras/models.py'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import keras.models\n",
    "reload(keras.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('model_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_COUNT = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataSet(object):\n",
    "    def __init__(self, \n",
    "                 file_path,\n",
    "                 word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
    "                 MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
    "                 text_col_name='text',\n",
    "                 label_col_name='tags',\n",
    "                 one_hot_encoder=None):\n",
    "        self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
    "\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.text_col_name = text_col_name\n",
    "        self.label_col_name = label_col_name\n",
    "\n",
    "        if label_col_name is not None:\n",
    "            self.label_encoder = self._fit_label_encoder(label_col_name)\n",
    "            self.onehot_encoder = self._fit_onehot_encoder()\n",
    "\n",
    "        if one_hot_encoder is not None:\n",
    "            self.one_hot_encoder = one_hot_encoder\n",
    "\n",
    "        self.embeddings = self._load_word_embeddings(word_embedding)\n",
    "\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "\n",
    "    def get_features(self, use_cache=True):        \n",
    "        if use_cache and self.features is not None:\n",
    "            return self.features\n",
    "        clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
    "        self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
    "        return self.features\n",
    "\n",
    "    def get_labels(self, use_cache=True):\n",
    "        if self.label_col_name is None:\n",
    "            raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
    "        if use_cache and self.labels is not None:\n",
    "            return self.labels\n",
    "        self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
    "        return self.labels\n",
    "\n",
    "    def _parse_text(self, text):\n",
    "        if isinstance(text, str):\n",
    "            text_parsed = Text(text)\n",
    "        else:\n",
    "            text_parsed = text\n",
    "        return text_parsed\n",
    "\n",
    "    def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
    "        if isinstance(article_text, str):\n",
    "            article_parsed = self._parse_text(article_text)\n",
    "\n",
    "        sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
    "        return sentences_words_embedding\n",
    "\n",
    "    def _load_word_embeddings(self, word_embedding):\n",
    "        if isinstance(word_embedding, Embedding):\n",
    "            return word_embedding\n",
    "        else:\n",
    "            return Embedding.load(word_embedding)\n",
    "\n",
    "    def _load_data_from_csv(self, file_path):\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "    def _get_clean_text_col(self, text_col):\n",
    "        \"\"\"remove html tags in text\"\"\"\n",
    "        text_col = self.df[text_col]\n",
    "        return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
    "\n",
    "    def _fit_label_encoder(self, label_col):\n",
    "        label_encoder = preprocessing.LabelEncoder()\n",
    "        label_encoder.fit(self.df[label_col].tolist())\n",
    "        self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
    "        self.label_encoder = label_encoder\n",
    "        return label_encoder\n",
    "\n",
    "    def _fit_onehot_encoder(self):\n",
    "        onehot_encoder = preprocessing.OneHotEncoder()\n",
    "        onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
    "        self.onehot_encoder = onehot_encoder\n",
    "        return onehot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TextClassificationDataSet(file_path='../data/offsite-tagging-test-set (1).csv', label_col_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_model.predict(dataset_test.get_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974/974 [==============================] - 7s     \n"
     ]
    }
   ],
   "source": [
    "pred_classes = best_model.predict_classes(dataset_test.get_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00779445,  0.03504602,  0.95715952],\n",
       "       [ 0.72062439,  0.16627042,  0.11310524],\n",
       "       [ 0.00546045,  0.02126656,  0.97327304],\n",
       "       ..., \n",
       "       [ 0.10732042,  0.17002803,  0.72265148],\n",
       "       [ 0.00891487,  0.04745243,  0.94363272],\n",
       "       [ 0.02163052,  0.07743791,  0.90093154]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dataset_test.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = dataset_train.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predicted_tags'] = label_encoder.inverse_transform(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td> Felix ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>$0 now8...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136</td>\n",
       "      <td> Big 55...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>213</td>\n",
       "      <td>01 ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>658</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>700</td>\n",
       "      <td> 0...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>729</td>\n",
       "      <td>19 3...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>837</td>\n",
       "      <td> 510...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1037</td>\n",
       "      <td>10 ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1095</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1113</td>\n",
       "      <td>A 2003...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1153</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1200</td>\n",
       "      <td>01 ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1261</td>\n",
       "      <td> 4130...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1332</td>\n",
       "      <td> \\r\\r\\n\\r\\r\\...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1374</td>\n",
       "      <td>01+C C...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1407</td>\n",
       "      <td>5 2010...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1564</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1643</td>\n",
       "      <td>37% ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1667</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1776</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1831</td>\n",
       "      <td>20 ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2011</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2036</td>\n",
       "      <td> 4...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2040</td>\n",
       "      <td>4 5844\\r\\r\\n ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2176</td>\n",
       "      <td> 13...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2221</td>\n",
       "      <td>FIFA7.7 1117...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2254</td>\n",
       "      <td>01 ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2287</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2327</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>91666</td>\n",
       "      <td>UGL UGL...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>91693</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>91699</td>\n",
       "      <td>01UGL \\r...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>91764</td>\n",
       "      <td> 11...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>91774</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>91887</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>92044</td>\n",
       "      <td>UGL UGL...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>92099</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>92124</td>\n",
       "      <td>UGL ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>92165</td>\n",
       "      <td>3 R&amp;F...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>92413</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>92455</td>\n",
       "      <td>80 3...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>92462</td>\n",
       "      <td>&lt;U+44EA&gt;  &amp;nbsp; &amp;nbsp...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>92494</td>\n",
       "      <td>UGL ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>92506</td>\n",
       "      <td>54 ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>92584</td>\n",
       "      <td>31 &lt;U+5421&gt;...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>92586</td>\n",
       "      <td>UGL UGL...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>92633</td>\n",
       "      <td>5 5...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>92706</td>\n",
       "      <td>5 20...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>92742</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>92744</td>\n",
       "      <td>UGL ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>92770</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>92817</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>92990</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>93063</td>\n",
       "      <td>UGL ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>93507</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>93651</td>\n",
       "      <td>&lt;U+615C&gt; ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>93690</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>93985</td>\n",
       "      <td> ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>94324</td>\n",
       "      <td>party ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text predicted_tags\n",
       "0        6   Felix ...             \n",
       "1      128  $0 now8...            \n",
       "2      136   Big 55...             \n",
       "3      213  01 ...             \n",
       "4      658   ...            \n",
       "5      700   0...            \n",
       "6      729  19 3...            \n",
       "7      837   510...             \n",
       "8     1037  10 ...             \n",
       "9     1095   ...             \n",
       "10    1113  A 2003...             \n",
       "11    1153   ...             \n",
       "12    1200  01 ...             \n",
       "13    1261   4130...             \n",
       "14    1332   \\r\\r\\n\\r\\r\\...             \n",
       "15    1374  01+C C...             \n",
       "16    1407  5 2010...             \n",
       "17    1564   ...            \n",
       "18    1643  37% ...            \n",
       "19    1667   ...            \n",
       "20    1776   ...           \n",
       "21    1831  20 ...            \n",
       "22    2011   ...            \n",
       "23    2036   4...            \n",
       "24    2040  4 5844\\r\\r\\n ...            \n",
       "25    2176   13...            \n",
       "26    2221  FIFA7.7 1117...             \n",
       "27    2254  01 ...            \n",
       "28    2287   ...             \n",
       "29    2327   ...            \n",
       "..     ...                                                ...            ...\n",
       "944  91666  UGL UGL...            \n",
       "945  91693   ...             \n",
       "946  91699  01UGL \\r...            \n",
       "947  91764   11...             \n",
       "948  91774   ...            \n",
       "949  91887   ...             \n",
       "950  92044  UGL UGL...            \n",
       "951  92099   ...             \n",
       "952  92124  UGL ...            \n",
       "953  92165  3 R&F...            \n",
       "954  92413   ...             \n",
       "955  92455  80 3...             \n",
       "956  92462  <U+44EA>  &nbsp; &nbsp...            \n",
       "957  92494  UGL ...            \n",
       "958  92506  54 ...             \n",
       "959  92584  31 <U+5421>...             \n",
       "960  92586  UGL UGL...            \n",
       "961  92633  5 5...             \n",
       "962  92706  5 20...             \n",
       "963  92742   ...             \n",
       "964  92744  UGL ...            \n",
       "965  92770   ...             \n",
       "966  92817   ...            \n",
       "967  92990   ...             \n",
       "968  93063  UGL ...            \n",
       "969  93507   ...             \n",
       "970  93651  <U+615C> ...             \n",
       "971  93690   ...             \n",
       "972  93985   ...             \n",
       "973  94324  party ...             \n",
       "\n",
       "[974 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('../output/testset_with_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
