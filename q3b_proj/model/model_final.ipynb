{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use polyglot for tokenizing and word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyglot\n",
    "from polyglot.text import Text, Word\n",
    "from polyglot.mapping import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use sklearn for the utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use keras with tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense, LSTM, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use hyperas for hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    hyperas has some weird error if the first_n_records or TextClassificationDataSet is parametrized... should refactor\n",
    "    \"\"\"\n",
    "    MAX_WORD_COUNT = 150\n",
    "    class TextClassificationDataSet(object):\n",
    "        def __init__(self, \n",
    "                     file_path,\n",
    "                     word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
    "                     MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
    "                     text_col_name='text',\n",
    "                     label_col_name='tags',\n",
    "                     one_hot_encoder=None):\n",
    "            self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
    "\n",
    "            self.df = pd.read_csv(file_path)\n",
    "            self.text_col_name = text_col_name\n",
    "            self.label_col_name = label_col_name\n",
    "\n",
    "            if label_col_name is not None:\n",
    "                self.label_encoder = self._fit_label_encoder(label_col_name)\n",
    "                self.onehot_encoder = self._fit_onehot_encoder()\n",
    "\n",
    "            if one_hot_encoder is not None:\n",
    "                self.one_hot_encoder = one_hot_encoder\n",
    "\n",
    "            self.embeddings = self._load_word_embeddings(word_embedding)\n",
    "\n",
    "            self.features = None\n",
    "            self.labels = None\n",
    "\n",
    "        def get_features(self, use_cache=True):        \n",
    "            if use_cache and self.features is not None:\n",
    "                return self.features\n",
    "            clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
    "            self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
    "            return self.features\n",
    "\n",
    "        def get_labels(self, use_cache=True):\n",
    "            if self.label_col_name is None:\n",
    "                raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
    "            if use_cache and self.labels is not None:\n",
    "                return self.labels\n",
    "            self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
    "            return self.labels\n",
    "\n",
    "        def _parse_text(self, text):\n",
    "            if isinstance(text, str):\n",
    "                text_parsed = Text(text)\n",
    "            else:\n",
    "                text_parsed = text\n",
    "            return text_parsed\n",
    "\n",
    "        def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
    "            if isinstance(article_text, str):\n",
    "                article_parsed = self._parse_text(article_text)\n",
    "\n",
    "            sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
    "            return sentences_words_embedding\n",
    "\n",
    "        def _load_word_embeddings(self, word_embedding):\n",
    "            if isinstance(word_embedding, Embedding):\n",
    "                return word_embedding\n",
    "            else:\n",
    "                return Embedding.load(word_embedding)\n",
    "\n",
    "        def _load_data_from_csv(self, file_path):\n",
    "            return pd.read_csv(file_path)\n",
    "\n",
    "        def _get_clean_text_col(self, text_col):\n",
    "            \"\"\"remove html tags in text\"\"\"\n",
    "            text_col = self.df[text_col]\n",
    "            return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
    "\n",
    "        def _fit_label_encoder(self, label_col):\n",
    "            label_encoder = preprocessing.LabelEncoder()\n",
    "            label_encoder.fit(self.df[label_col].tolist())\n",
    "            self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
    "            self.label_encoder = label_encoder\n",
    "            return label_encoder\n",
    "\n",
    "        def _fit_onehot_encoder(self):\n",
    "            onehot_encoder = preprocessing.OneHotEncoder()\n",
    "            onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
    "            self.onehot_encoder = onehot_encoder\n",
    "            return onehot_encoder\n",
    "    dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(dataset_train.get_features(), dataset_train.get_labels(), test_size=0.2, random_state=42)\n",
    "    return X_train, y_train, X_validate, y_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_first1024():\n",
    "    \"\"\" \n",
    "    same as data(), just that it only returns the first 1024 rows of the input \n",
    "    => faster for searching hyper params\n",
    "    \n",
    "    hyperas has some weird error if the first_n_records or TextClassificationDataSet is parametrized... should refactor\n",
    "    \"\"\"\n",
    "    MAX_WORD_COUNT = 150\n",
    "    first_n_records = 1024\n",
    "    class TextClassificationDataSet(object):\n",
    "        def __init__(self, \n",
    "                     file_path,\n",
    "                     word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
    "                     MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
    "                     text_col_name='text',\n",
    "                     label_col_name='tags',\n",
    "                     one_hot_encoder=None):\n",
    "            self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
    "\n",
    "            self.df = pd.read_csv(file_path).head(first_n_records)\n",
    "            self.text_col_name = text_col_name\n",
    "            self.label_col_name = label_col_name\n",
    "\n",
    "            if label_col_name is not None:\n",
    "                self.label_encoder = self._fit_label_encoder(label_col_name)\n",
    "                self.onehot_encoder = self._fit_onehot_encoder()\n",
    "\n",
    "            if one_hot_encoder is not None:\n",
    "                self.one_hot_encoder = one_hot_encoder\n",
    "\n",
    "            self.embeddings = self._load_word_embeddings(word_embedding)\n",
    "\n",
    "            self.features = None\n",
    "            self.labels = None\n",
    "\n",
    "        def get_features(self, use_cache=True):\n",
    "            if use_cache and self.features is not None:\n",
    "                return self.features\n",
    "            clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
    "            self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
    "            return self.features\n",
    "\n",
    "        def get_labels(self, use_cache=True):\n",
    "            if self.label_col_name is None:\n",
    "                raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
    "            if use_cache and self.labels is not None:\n",
    "                return self.labels\n",
    "            self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
    "            return self.labels\n",
    "\n",
    "        def _parse_text(self, text):\n",
    "            if isinstance(text, str):\n",
    "                text_parsed = Text(text)\n",
    "            else:\n",
    "                text_parsed = text\n",
    "            return text_parsed\n",
    "\n",
    "        def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
    "            if isinstance(article_text, str):\n",
    "                article_parsed = self._parse_text(article_text)\n",
    "\n",
    "            sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
    "            return sentences_words_embedding\n",
    "\n",
    "        def _load_word_embeddings(self, word_embedding):\n",
    "            if isinstance(word_embedding, Embedding):\n",
    "                return word_embedding\n",
    "            else:\n",
    "                return Embedding.load(word_embedding)\n",
    "\n",
    "        def _load_data_from_csv(self, file_path):\n",
    "            return pd.read_csv(file_path)\n",
    "\n",
    "        def _get_clean_text_col(self, text_col):\n",
    "            \"\"\"remove html tags in text\"\"\"\n",
    "            text_col = self.df[text_col]\n",
    "            return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
    "\n",
    "        def _fit_label_encoder(self, label_col):\n",
    "            label_encoder = preprocessing.LabelEncoder()\n",
    "            label_encoder.fit(self.df[label_col].tolist())\n",
    "            self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
    "            self.label_encoder = label_encoder\n",
    "            return label_encoder\n",
    "\n",
    "        def _fit_onehot_encoder(self):\n",
    "            onehot_encoder = preprocessing.OneHotEncoder()\n",
    "            onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
    "            self.onehot_encoder = onehot_encoder\n",
    "            return onehot_encoder\n",
    "    dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(dataset_train.get_features(), dataset_train.get_labels(), test_size=0.2, random_state=42)\n",
    "    return X_train, y_train, X_validate, y_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_model_seq(X_train, y_train, X_validate, y_validate):\n",
    "    \"\"\"\n",
    "    Defines the computational graph.\n",
    "    \"\"\"\n",
    "    MAX_WORD_COUNT = 150\n",
    "    embedding_size = 64\n",
    "    tag_classes_count = 3\n",
    "    \n",
    "    batch_size = {{choice([128, 256, 512])}}\n",
    "    lstm_units = {{choice([64, 128, 256, 512])}}\n",
    "    dense_units = {{choice([64, 128, 256, 512])}}\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(lstm_units, input_shape=(MAX_WORD_COUNT, embedding_size), name='LSTM'))\n",
    "    \n",
    "    model.add(Dense(dense_units, activation='relu', name='Dense_1'))\n",
    "    model.add(Dense(dense_units, activation='relu', name='Dense_2'))\n",
    "    model.add(Dense(dense_units, activation='relu', name='Dense_3'))\n",
    "\n",
    "    model.add(Dense(tag_classes_count, activation='softmax', name='main_output'))\n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam', 'adagrad', 'nadam', 'adadelta'])}}, \n",
    "              loss={'main_output': 'categorical_crossentropy'}, \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              validation_data=(X_validate, y_validate))\n",
    "    \n",
    "    score, acc = model.evaluate(X_validate, y_validate, batch_size=batch_size, verbose=0)\n",
    "    print('Test Accuracy:{}'.format(acc))\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_validate, y_validate = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from bs4 import BeautifulSoup\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import polyglot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from polyglot.text import Text, Word\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from polyglot.mapping import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, LSTM, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model, Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import optimizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe, rand\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'batch_size': hp.choice('batch_size', [128, 256, 512]),\n",
      "        'lstm_units': hp.choice('lstm_units', [64, 128, 256, 512]),\n",
      "        'lstm_units_1': hp.choice('lstm_units_1', [64, 128, 256, 512]),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'adagrad', 'nadam', 'adadelta']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: \"\"\" \n",
      "   3: same as data(), just that it only returns the first 1024 rows of the input \n",
      "   4: => faster for searching hyper params\n",
      "   5: \n",
      "   6: hyperas has some weird error if the first_n_records or TextClassificationDataSet is parametrized... should refactor\n",
      "   7: \"\"\"\n",
      "   8: MAX_WORD_COUNT = 150\n",
      "   9: first_n_records = 1024\n",
      "  10: class TextClassificationDataSet(object):\n",
      "  11:     def __init__(self, \n",
      "  12:                  file_path,\n",
      "  13:                  word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
      "  14:                  MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
      "  15:                  text_col_name='text',\n",
      "  16:                  label_col_name='tags',\n",
      "  17:                  one_hot_encoder=None):\n",
      "  18:         self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
      "  19: \n",
      "  20:         self.df = pd.read_csv(file_path).head(first_n_records)\n",
      "  21:         self.text_col_name = text_col_name\n",
      "  22:         self.label_col_name = label_col_name\n",
      "  23: \n",
      "  24:         if label_col_name is not None:\n",
      "  25:             self.label_encoder = self._fit_label_encoder(label_col_name)\n",
      "  26:             self.onehot_encoder = self._fit_onehot_encoder()\n",
      "  27: \n",
      "  28:         if one_hot_encoder is not None:\n",
      "  29:             self.one_hot_encoder = one_hot_encoder\n",
      "  30: \n",
      "  31:         self.embeddings = self._load_word_embeddings(word_embedding)\n",
      "  32: \n",
      "  33:         self.features = None\n",
      "  34:         self.labels = None\n",
      "  35: \n",
      "  36:     def get_features(self, use_cache=True):\n",
      "  37:         if use_cache and self.features is not None:\n",
      "  38:             return self.features\n",
      "  39:         clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
      "  40:         self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
      "  41:         return self.features\n",
      "  42: \n",
      "  43:     def get_labels(self, use_cache=True):\n",
      "  44:         if self.label_col_name is None:\n",
      "  45:             raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
      "  46:         if use_cache and self.labels is not None:\n",
      "  47:             return self.labels\n",
      "  48:         self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
      "  49:         return self.labels\n",
      "  50: \n",
      "  51:     def _parse_text(self, text):\n",
      "  52:         if isinstance(text, str):\n",
      "  53:             text_parsed = Text(text)\n",
      "  54:         else:\n",
      "  55:             text_parsed = text\n",
      "  56:         return text_parsed\n",
      "  57: \n",
      "  58:     def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
      "  59:         if isinstance(article_text, str):\n",
      "  60:             article_parsed = self._parse_text(article_text)\n",
      "  61: \n",
      "  62:         sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
      "  63:         return sentences_words_embedding\n",
      "  64: \n",
      "  65:     def _load_word_embeddings(self, word_embedding):\n",
      "  66:         if isinstance(word_embedding, Embedding):\n",
      "  67:             return word_embedding\n",
      "  68:         else:\n",
      "  69:             return Embedding.load(word_embedding)\n",
      "  70: \n",
      "  71:     def _load_data_from_csv(self, file_path):\n",
      "  72:         return pd.read_csv(file_path)\n",
      "  73: \n",
      "  74:     def _get_clean_text_col(self, text_col):\n",
      "  75:         \"\"\"remove html tags in text\"\"\"\n",
      "  76:         text_col = self.df[text_col]\n",
      "  77:         return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
      "  78: \n",
      "  79:     def _fit_label_encoder(self, label_col):\n",
      "  80:         label_encoder = preprocessing.LabelEncoder()\n",
      "  81:         label_encoder.fit(self.df[label_col].tolist())\n",
      "  82:         self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
      "  83:         self.label_encoder = label_encoder\n",
      "  84:         return label_encoder\n",
      "  85: \n",
      "  86:     def _fit_onehot_encoder(self):\n",
      "  87:         onehot_encoder = preprocessing.OneHotEncoder()\n",
      "  88:         onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
      "  89:         self.onehot_encoder = onehot_encoder\n",
      "  90:         return onehot_encoder\n",
      "  91: dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')\n",
      "  92: X_train, X_validate, y_train, y_validate = train_test_split(dataset_train.get_features(), dataset_train.get_labels(), test_size=0.2, random_state=42)\n",
      "  93: \n",
      "  94: \n",
      "  95: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Defines the computational graph.\n",
      "   5:     \"\"\"\n",
      "   6:     MAX_WORD_COUNT = 150\n",
      "   7:     embedding_size = 64\n",
      "   8:     tag_classes_count = 3\n",
      "   9:     \n",
      "  10:     batch_size = space['batch_size']\n",
      "  11:     lstm_units = space['lstm_units']\n",
      "  12:     dense_units = space['lstm_units_1']\n",
      "  13:     \n",
      "  14:     model = Sequential()\n",
      "  15: \n",
      "  16:     model.add(LSTM(lstm_units, input_shape=(MAX_WORD_COUNT, embedding_size), name='LSTM'))\n",
      "  17:     \n",
      "  18:     model.add(Dense(dense_units, activation='relu', name='Dense_1'))\n",
      "  19:     model.add(Dense(dense_units, activation='relu', name='Dense_2'))\n",
      "  20:     model.add(Dense(dense_units, activation='relu', name='Dense_3'))\n",
      "  21: \n",
      "  22:     model.add(Dense(tag_classes_count, activation='softmax', name='main_output'))\n",
      "  23:     model.compile(optimizer=space['optimizer'], \n",
      "  24:               loss={'main_output': 'categorical_crossentropy'}, \n",
      "  25:               metrics=['accuracy'])\n",
      "  26:     \n",
      "  27:     model.fit(X_train, y_train,\n",
      "  28:               batch_size=batch_size,\n",
      "  29:               epochs=1,\n",
      "  30:               validation_data=(X_validate, y_validate))\n",
      "  31:     \n",
      "  32:     score, acc = model.evaluate(X_validate, y_validate, batch_size=batch_size, verbose=0)\n",
      "  33:     print('Test Accuracy:{}'.format(acc))\n",
      "  34:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  35: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 16s - loss: 1.0613 - acc: 0.5128 - val_loss: 0.9584 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0865 - acc: 0.4396 - val_loss: 1.0125 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 4s - loss: 1.7196 - acc: 0.4921 - val_loss: 1.0549 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 42s - loss: 1.0126 - acc: 0.4969 - val_loss: 0.8924 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 42s - loss: 1.0726 - acc: 0.3871 - val_loss: 1.5691 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 15s - loss: 3.5338 - acc: 0.3541 - val_loss: 6.9976 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 5s - loss: 0.9810 - acc: 0.5165 - val_loss: 0.8653 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 44s - loss: 5.5780 - acc: 0.4591 - val_loss: 6.9976 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 39s - loss: 1.0948 - acc: 0.3480 - val_loss: 1.0441 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 41s - loss: 1.1116 - acc: 0.4737 - val_loss: 1.0387 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 40s - loss: 1.0557 - acc: 0.4957 - val_loss: 1.0577 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 4s - loss: 1.0886 - acc: 0.4322 - val_loss: 1.0591 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 41s - loss: 1.0690 - acc: 0.3443 - val_loss: 2.2899 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 6s - loss: 1.0458 - acc: 0.4469 - val_loss: 0.9821 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 5s - loss: 1.0386 - acc: 0.4933 - val_loss: 0.9717 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 5s - loss: 1.0833 - acc: 0.4310 - val_loss: 1.0147 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0440 - acc: 0.5128 - val_loss: 0.9504 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 40s - loss: 1.1034 - acc: 0.4188 - val_loss: 1.0460 - val_acc: 0.5415\n",
      "Test Accuracy:0.5414634172509356\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 4s - loss: 1.0907 - acc: 0.4261 - val_loss: 1.0575 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 5s - loss: 1.0297 - acc: 0.5226 - val_loss: 0.9633 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0336 - acc: 0.5519 - val_loss: 0.9579 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0680 - acc: 0.4554 - val_loss: 0.9858 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0530 - acc: 0.5031 - val_loss: 0.9668 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0707 - acc: 0.4811 - val_loss: 0.9996 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0495 - acc: 0.5140 - val_loss: 0.9704 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 9s - loss: 1.0324 - acc: 0.5531 - val_loss: 0.9651 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 8s - loss: 1.0337 - acc: 0.5531 - val_loss: 0.9594 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0577 - acc: 0.5165 - val_loss: 0.9909 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 19s - loss: 1.1065 - acc: 0.5140 - val_loss: 0.8791 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.0091 - acc: 0.5507 - val_loss: 0.9263 - val_acc: 0.6244\n",
      "Test Accuracy:0.6243902328537732\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.0564 - acc: 0.4957 - val_loss: 0.9628 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 18s - loss: 1.0916 - acc: 0.4860 - val_loss: 0.9580 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.0457 - acc: 0.5018 - val_loss: 0.9305 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 18s - loss: 1.1526 - acc: 0.5226 - val_loss: 0.9271 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 18s - loss: 1.0091 - acc: 0.4957 - val_loss: 0.8861 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 18s - loss: 1.1385 - acc: 0.5055 - val_loss: 0.9386 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.0986 - acc: 0.5421 - val_loss: 0.9551 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.3883 - acc: 0.4957 - val_loss: 0.9116 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 6s - loss: 1.0048 - acc: 0.5531 - val_loss: 0.9466 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0736 - acc: 0.5507 - val_loss: 1.0055 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819/819 [==============================] - 6s - loss: 1.0226 - acc: 0.5495 - val_loss: 0.9821 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 9s - loss: 2.5498 - acc: 0.5409 - val_loss: 1.0489 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 9s - loss: 1.0165 - acc: 0.5519 - val_loss: 0.9368 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 17s - loss: 1.2276 - acc: 0.4481 - val_loss: 0.9981 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 44s - loss: 1.0802 - acc: 0.5531 - val_loss: 1.0413 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 11s - loss: 6.3304 - acc: 0.5250 - val_loss: 6.9976 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.2694 - acc: 0.4957 - val_loss: 0.9899 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 43s - loss: 1.2028 - acc: 0.5275 - val_loss: 1.0174 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0977 - acc: 0.3700 - val_loss: 1.0523 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.3056 - acc: 0.4420 - val_loss: 0.9941 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 45s - loss: 1.0520 - acc: 0.5067 - val_loss: 0.9669 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.5201 - acc: 0.4017 - val_loss: 1.0245 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 19s - loss: 1.0543 - acc: 0.3553 - val_loss: 1.6306 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0228 - acc: 0.5531 - val_loss: 0.9628 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 44s - loss: 1.9597 - acc: 0.4505 - val_loss: 1.0157 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 10s - loss: 1.0535 - acc: 0.5250 - val_loss: 0.9788 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.0031 - acc: 0.5287 - val_loss: 0.9299 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 19s - loss: 1.1240 - acc: 0.5372 - val_loss: 1.0775 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 11s - loss: 1.0428 - acc: 0.5275 - val_loss: 0.9531 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 7s - loss: 1.0390 - acc: 0.4872 - val_loss: 0.9280 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 45s - loss: 1.0444 - acc: 0.4969 - val_loss: 0.9633 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 19s - loss: 1.0460 - acc: 0.5458 - val_loss: 0.9754 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 21s - loss: 1.0775 - acc: 0.3602 - val_loss: 0.9774 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.0294 - acc: 0.4872 - val_loss: 0.9965 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 21s - loss: 1.1758 - acc: 0.5495 - val_loss: 0.9937 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536553382874\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.0211 - acc: 0.4786 - val_loss: 1.0020 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 141s - loss: 1.0362 - acc: 0.5226 - val_loss: 0.9478 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 32s - loss: 1.0138 - acc: 0.4957 - val_loss: 0.8979 - val_acc: 0.5707\n",
      "Test Accuracy:0.5707317099338625\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 20s - loss: 1.1184 - acc: 0.5372 - val_loss: 0.9242 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n",
      "Train on 819 samples, validate on 205 samples\n",
      "Epoch 1/1\n",
      "819/819 [==============================] - 1696s - loss: 1.0838 - acc: 0.4212 - val_loss: 0.9571 - val_acc: 0.5659\n",
      "Test Accuracy:0.5658536611533747\n"
     ]
    }
   ],
   "source": [
    "# import gc; gc.collect()\n",
    "trials = Trials()\n",
    "best_run, best_model, space = optim.minimize(model=search_model_seq,\n",
    "                                      data=data_first1024,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=70,\n",
    "                                      trials=trials,\n",
    "                                      notebook_name='model_final',\n",
    "                                             eval_space=True,   # <-- this is the line that puts real values into 'best_run'\n",
    "                                             return_space=True  # <-- this allows you to save the space for later evaluations \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "779/779 [==============================] - 6s     \n",
      "[0.94468809658510844, 0.59563543026805388]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'batch_size': 128, 'lstm_units': 256, 'lstm_units_1': 256, 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_validate, y_validate))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'lstm_units': 256,\n",
       " 'lstm_units_1': 256,\n",
       " 'optimizer': 'rmsprop'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train,\n",
    "              batch_size=best_run['batch_size'],\n",
    "              epochs=20,\n",
    "#               verbose=2,\n",
    "              validation_data=(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 [==============================] - 5s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27436906497193853, 0.91014120659871034]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.models' from '/Users/ericng/Workspace/hk01_test/q3b_proj/model/tag-clf/lib/python3.6/site-packages/keras/models.py'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import keras.models\n",
    "reload(keras.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save('model_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_COUNT = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataSet(object):\n",
    "    def __init__(self, \n",
    "                 file_path,\n",
    "                 word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
    "                 MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
    "                 text_col_name='text',\n",
    "                 label_col_name='tags',\n",
    "                 one_hot_encoder=None):\n",
    "        self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
    "\n",
    "        self.df = pd.read_csv(file_path)\n",
    "        self.text_col_name = text_col_name\n",
    "        self.label_col_name = label_col_name\n",
    "\n",
    "        if label_col_name is not None:\n",
    "            self.label_encoder = self._fit_label_encoder(label_col_name)\n",
    "            self.onehot_encoder = self._fit_onehot_encoder()\n",
    "\n",
    "        if one_hot_encoder is not None:\n",
    "            self.one_hot_encoder = one_hot_encoder\n",
    "\n",
    "        self.embeddings = self._load_word_embeddings(word_embedding)\n",
    "\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "\n",
    "    def get_features(self, use_cache=True):        \n",
    "        if use_cache and self.features is not None:\n",
    "            return self.features\n",
    "        clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
    "        self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
    "        return self.features\n",
    "\n",
    "    def get_labels(self, use_cache=True):\n",
    "        if self.label_col_name is None:\n",
    "            raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
    "        if use_cache and self.labels is not None:\n",
    "            return self.labels\n",
    "        self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
    "        return self.labels\n",
    "\n",
    "    def _parse_text(self, text):\n",
    "        if isinstance(text, str):\n",
    "            text_parsed = Text(text)\n",
    "        else:\n",
    "            text_parsed = text\n",
    "        return text_parsed\n",
    "\n",
    "    def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
    "        if isinstance(article_text, str):\n",
    "            article_parsed = self._parse_text(article_text)\n",
    "\n",
    "        sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
    "        return sentences_words_embedding\n",
    "\n",
    "    def _load_word_embeddings(self, word_embedding):\n",
    "        if isinstance(word_embedding, Embedding):\n",
    "            return word_embedding\n",
    "        else:\n",
    "            return Embedding.load(word_embedding)\n",
    "\n",
    "    def _load_data_from_csv(self, file_path):\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "    def _get_clean_text_col(self, text_col):\n",
    "        \"\"\"remove html tags in text\"\"\"\n",
    "        text_col = self.df[text_col]\n",
    "        return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
    "\n",
    "    def _fit_label_encoder(self, label_col):\n",
    "        label_encoder = preprocessing.LabelEncoder()\n",
    "        label_encoder.fit(self.df[label_col].tolist())\n",
    "        self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
    "        self.label_encoder = label_encoder\n",
    "        return label_encoder\n",
    "\n",
    "    def _fit_onehot_encoder(self):\n",
    "        onehot_encoder = preprocessing.OneHotEncoder()\n",
    "        onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
    "        self.onehot_encoder = onehot_encoder\n",
    "        return onehot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TextClassificationDataSet(file_path='../data/offsite-tagging-test-set (1).csv', label_col_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = best_model.predict(dataset_test.get_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974/974 [==============================] - 7s     \n"
     ]
    }
   ],
   "source": [
    "pred_classes = best_model.predict_classes(dataset_test.get_features())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00779445,  0.03504602,  0.95715952],\n",
       "       [ 0.72062439,  0.16627042,  0.11310524],\n",
       "       [ 0.00546045,  0.02126656,  0.97327304],\n",
       "       ..., \n",
       "       [ 0.10732042,  0.17002803,  0.72265148],\n",
       "       [ 0.00891487,  0.04745243,  0.94363272],\n",
       "       [ 0.02163052,  0.07743791,  0.90093154]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = dataset_test.df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = dataset_train.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['predicted_tags'] = label_encoder.inverse_transform(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>predicted_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>南華添鋒力　簽前厄瓜多爾國腳保耶 港超勁旅南華宣布羅致前厄瓜多爾國腳菲力斯保耶（Felix ...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>如果大學$0捐款　科大嶺南將蝕過千萬元 據now新聞台報道，身兼8大校監的特首梁振英曾以大學...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>136</td>\n",
       "      <td>英超最強火力對碰　雙城爭冠靠鋒霸 英超今季風起雲湧，傳統「Big 5」只剩兩隊名列積分榜前5...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>213</td>\n",
       "      <td>【01球評】膺半程冠軍　阿仙奴今季不奪標更待何時？ 近年「兵工廠」每季的起落都離不開一個循環...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>658</td>\n",
       "      <td>【書商失蹤】梁振英：希望失蹤的李波本人提供資料 行政長官梁振英出席行政會議前見記者，被問及李...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>700</td>\n",
       "      <td>【施政盤點】三份施政報告　僅一半政策達標 行政長官梁振英即將公布任內第四份施政報告，《香港0...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>729</td>\n",
       "      <td>【施政盤點】「治港絕招」　設19委員會　空談多實務少 行政長官梁振英上任3年多，先後成立多個...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>837</td>\n",
       "      <td>高普首簽　「新馬迪」來季投紅軍 利物浦傷兵滿營及戰績不穩，主帥高普仍不忘投資未來，昨以510...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1037</td>\n",
       "      <td>「最潮主帥」鬥利物浦：我已領先1：0 英乙球隊埃克塞特在明晨的足總盃於主場迎戰利物浦，雖然越...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1095</td>\n",
       "      <td>紅軍超殘陣逼和英乙隊　高普：負擔不起重賽 逾十名球員受傷的利物浦，今晨在足總盃第三圈以大部份...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1113</td>\n",
       "      <td>【施丹上馬】皇馬六條A　退下來各自精彩　僅卡路士教波 2003年碧咸轉投皇家馬德里，與施丹、...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1153</td>\n",
       "      <td>【施丹上馬】踢而優則教　碧根鮑華告魯夫完美球王 足球史上，獲譽為球王的寥寥可數，踢而優則教的...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1200</td>\n",
       "      <td>【01球評】施丹首戰　回歸原點抄足肥安　防守毛病未解決 所謂「新官上任三把火」，不過從皇家馬...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1261</td>\n",
       "      <td>新兵白鶴對辦　東方大破南華踞榜首現霸氣 星期日下午的港超榜首大戰，東方在4130名觀眾面前盡...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1332</td>\n",
       "      <td>【意甲半程總結】四軍混戰　拿玻里勢破祖記壟斷 兵多將廣　拿玻里攻上榜首\\r\\r\\n\\r\\r\\...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1374</td>\n",
       "      <td>【01球評】協同效應+品牌角力　美斯C朗壟斷金球獎 「美斯定C朗？」球迷間討論的問題，在明晨...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1407</td>\n",
       "      <td>金球獎合併5年　首屆美斯得獎爭議最大 自國際足協在2010年把世界足球先生及《法國足球》的歐...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1564</td>\n",
       "      <td>梁振英批司法覆核遭濫用　對政府代價大 現任終院首席法官馬道立日前強調，司法覆核維護公眾利益，...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1643</td>\n",
       "      <td>【港大民調】支持度僅37%創新低　數據顯示董曾梁一蟹不如一蟹 特首梁振英昨日宣讀施政報告，香...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1667</td>\n",
       "      <td>【獨家民調】梁治三年　市民最不滿政制司法　最滿意房屋規劃 今年的施政報告重點力推一帶一路經濟...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1776</td>\n",
       "      <td>奧巴馬炮轟特朗普耍選戰手段　承認未能團結美國 美國總統奧巴馬在電視節目上，批評共和黨總統參選...</td>\n",
       "      <td>美國大選</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1831</td>\n",
       "      <td>【施政報告】預留20億元成立創科創投基金 行政長官梁振英在新一份施政報告提到本港創新科技發展...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2011</td>\n",
       "      <td>由象徵式政策到象徵式施政報告 師父教落，自我介紹中說出來的，通常都不重要，沒有說出來的才重要...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2036</td>\n",
       "      <td>【施政報告】工聯會考慮不支持梁振英　梁連任夢添陰霾 行政長官梁振英發表任內第4份《施政報告》...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2040</td>\n",
       "      <td>港美同日發表施政藍圖　梁振英奧巴馬4大看點 演說長度奧巴馬　58分44秒\\r\\r\\n梁振英 ...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2176</td>\n",
       "      <td>【獨家民調】青年人拒撐梁　無學生支持梁振英連任特首 梁振英昨日（13日）發表《施政報告》後，...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2221</td>\n",
       "      <td>港中戰球迷噓國歌　FIFA再罰香港足總7.7萬 去年「11‧17」港中大戰餘波未了，國際足協...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2254</td>\n",
       "      <td>【01拆局】曾俊華民望雖高　未獲左派認可　與泛民友好成雙刃劍 特首梁振英新一份施政報告劣評如...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2287</td>\n",
       "      <td>【港足日與夜】從日與夜開始　了解五個香港足球員的故事 經歷去年世界盃外圍賽的熱潮，香港足球再...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2327</td>\n",
       "      <td>【施政報告】梁振英：即使當年有普選我仍會當選 今年《施政報告》被指「區議會化」，及變成「一帶...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>91666</td>\n",
       "      <td>【梁振英UGL案．博評】梁周瓜田李下與議會歪風 梁振英、周浩鼎因UGL調查而鬧得輿論一片沸沸...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>91693</td>\n",
       "      <td>【趣聞】女足球員打架誤一生　毆打對手被終身禁賽 有時候一次衝動，足以改變一生。早前波斯尼亞女...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>91699</td>\n",
       "      <td>【01觀點．梁振英UGL案】調查演成爛局　要解釋的不止周浩鼎 若非其身不正　何不大方受查\\r...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>91764</td>\n",
       "      <td>【足球明星選舉】費蘭度奪四大獎項　朱志光首嘗最佳教練 11支球隊經過一整季的努力，港超聯決出...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>91774</td>\n",
       "      <td>【拆局】夕陽政府再打郊野公園主意　梁振英搞邊科？ 房協應政府邀請，研究在郊野公園邊陲地帶興建...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>91887</td>\n",
       "      <td>【英冠附加賽】高普伴郎率利記借將發功　哈特斯菲爾德晉級鬥雷丁 英超戰局大定，英冠則仍然有一場...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>92044</td>\n",
       "      <td>【拆局】梁振英循「正途」去信委員會　研UGL文件真偽有利案情？ 梁振英今日下午去信調查UGL...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>92099</td>\n",
       "      <td>【英超】熱刺大炒李斯特城　卡尼大四喜有望衛冕神射手（有片） 熱刺作客憑哈利卡尼大四喜加上孫興...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>92124</td>\n",
       "      <td>【UGL案】周浩鼎宣布即時辭任委員會委員　重申無違規違法無隱瞞 讓特首梁振英修改交予立法會調...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>92165</td>\n",
       "      <td>【港足日與夜．富力3】由中超降格港超　忍受球迷辱罵是成長特訓 R&amp;F富力留在港超踢波被罵得狗...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>92413</td>\n",
       "      <td>【德甲】韋比女友來季成德甲主球證　創歐洲主流聯賽歷史（有片） 德甲來季少了拿姆及沙比阿朗素，...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>92455</td>\n",
       "      <td>【港超】南華連續三季四大皆空　錯在球隊停在80年代？ 南華自班主張廣勇入主後，連續3季班費都...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>92462</td>\n",
       "      <td>梁振英送蘭花　證青協總幹事王&lt;U+44EA&gt;鳴退休 青協職員稱不捨 &amp;nbsp; &amp;nbsp...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>92494</td>\n",
       "      <td>【UGL案】謝偉俊稱無機制換走梁繼昌　集體決定無人可隻手遮天 行政長官梁振英經民建聯議員周浩...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>92506</td>\n",
       "      <td>【德甲】漢堡絕殺反勝禾夫斯堡　連續54季未降班保招牌（有片） 漢堡與禾夫斯堡兩支德甲老牌球隊...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>92584</td>\n",
       "      <td>【港超附加賽】南區3：1慘勝元朗　折多員大將下周決賽撼東方 地區打&lt;U+5421&gt;再次於港超...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>92586</td>\n",
       "      <td>【梁振英UGL案．博評】鼎，請鍾樹根回去做議員吧 【梁振英UGL案】近期熱爆政界的周浩鼎——...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>92633</td>\n",
       "      <td>【西甲】皇馬壓巴塞5年首奪西甲　施丹獲贈香檳浴（有片） 5年的等待，終於由施丹達成力壓巴塞隆...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>92706</td>\n",
       "      <td>【英超】阿仙奴成史上最高分第5名　雲格未定下季去留 雲格執教阿仙奴20年，首次未能帶領球隊獲...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>92742</td>\n",
       "      <td>【港超】盧卡斯率先轉會傑志　有望周五鬥熱刺 今季為和富大埔贏得菁英盃的功臣盧卡斯，一直盛傳會...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>92744</td>\n",
       "      <td>【UGL案】梁振英繼續狙擊梁繼昌　指無回應質疑「道理何在」？ 梁繼昌在多名民主派議員陪同下召...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>92770</td>\n",
       "      <td>【英超】泰利盧卡斯薩巴列達說再見　別了熟悉的面孔（有片） 一代新人換舊人，下一季英超，追捧多...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>92817</td>\n",
       "      <td>梁振英罕有推小冊子晒政績　尹兆堅：他想幫自己政治上風光大葬 本屆政府臨近尾聲，行政長官梁振英...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>92990</td>\n",
       "      <td>【熱刺訪港】卡尼阿里簽名不停手　普捷天奴揚言下季爭英超 應屆英超亞軍熱刺周二抵港，準備周五於...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>93063</td>\n",
       "      <td>【政圈風聲】UGL餘波愈演愈烈　林鄭或淪另類受害者？ 「浩鼎門」一石激起千重浪，梁振英連日與...</td>\n",
       "      <td>梁振英</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>93507</td>\n",
       "      <td>【熱刺訪港】普捷天奴成搶手貨　主席利維開腔派定心丸 英超今季群雄割據，摩連奴、干地及哥迪奧拿...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>93651</td>\n",
       "      <td>【熱刺訪港】孫興&lt;U+615C&gt;林志堅再聚舊　承諾賽後交換球衣 熱刺周五將與傑志於香港大球場...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>93690</td>\n",
       "      <td>【港足日與夜．王振鵬】膠唔會膠一世　神經刀變神龍（有片） 有些球員出道十多年，一直都被人睇低...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>93985</td>\n",
       "      <td>【中超】泰維斯抱怨遭中超球員踢傷　澄清無意離開上海申花 受人錢財不一定替人消災，阿根廷前鋒泰...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>94324</td>\n",
       "      <td>【傑志對熱刺．來稿】睇波睇到開party咁先至過癮 剛過去的周末，香港刮起一股足球熱。先是上...</td>\n",
       "      <td>足球</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>974 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text predicted_tags\n",
       "0        6  南華添鋒力　簽前厄瓜多爾國腳保耶 港超勁旅南華宣布羅致前厄瓜多爾國腳菲力斯保耶（Felix ...             足球\n",
       "1      128  如果大學$0捐款　科大嶺南將蝕過千萬元 據now新聞台報道，身兼8大校監的特首梁振英曾以大學...            梁振英\n",
       "2      136  英超最強火力對碰　雙城爭冠靠鋒霸 英超今季風起雲湧，傳統「Big 5」只剩兩隊名列積分榜前5...             足球\n",
       "3      213  【01球評】膺半程冠軍　阿仙奴今季不奪標更待何時？ 近年「兵工廠」每季的起落都離不開一個循環...             足球\n",
       "4      658  【書商失蹤】梁振英：希望失蹤的李波本人提供資料 行政長官梁振英出席行政會議前見記者，被問及李...            梁振英\n",
       "5      700  【施政盤點】三份施政報告　僅一半政策達標 行政長官梁振英即將公布任內第四份施政報告，《香港0...            梁振英\n",
       "6      729  【施政盤點】「治港絕招」　設19委員會　空談多實務少 行政長官梁振英上任3年多，先後成立多個...            梁振英\n",
       "7      837  高普首簽　「新馬迪」來季投紅軍 利物浦傷兵滿營及戰績不穩，主帥高普仍不忘投資未來，昨以510...             足球\n",
       "8     1037  「最潮主帥」鬥利物浦：我已領先1：0 英乙球隊埃克塞特在明晨的足總盃於主場迎戰利物浦，雖然越...             足球\n",
       "9     1095  紅軍超殘陣逼和英乙隊　高普：負擔不起重賽 逾十名球員受傷的利物浦，今晨在足總盃第三圈以大部份...             足球\n",
       "10    1113  【施丹上馬】皇馬六條A　退下來各自精彩　僅卡路士教波 2003年碧咸轉投皇家馬德里，與施丹、...             足球\n",
       "11    1153  【施丹上馬】踢而優則教　碧根鮑華告魯夫完美球王 足球史上，獲譽為球王的寥寥可數，踢而優則教的...             足球\n",
       "12    1200  【01球評】施丹首戰　回歸原點抄足肥安　防守毛病未解決 所謂「新官上任三把火」，不過從皇家馬...             足球\n",
       "13    1261  新兵白鶴對辦　東方大破南華踞榜首現霸氣 星期日下午的港超榜首大戰，東方在4130名觀眾面前盡...             足球\n",
       "14    1332  【意甲半程總結】四軍混戰　拿玻里勢破祖記壟斷 兵多將廣　拿玻里攻上榜首\\r\\r\\n\\r\\r\\...             足球\n",
       "15    1374  【01球評】協同效應+品牌角力　美斯C朗壟斷金球獎 「美斯定C朗？」球迷間討論的問題，在明晨...             足球\n",
       "16    1407  金球獎合併5年　首屆美斯得獎爭議最大 自國際足協在2010年把世界足球先生及《法國足球》的歐...             足球\n",
       "17    1564  梁振英批司法覆核遭濫用　對政府代價大 現任終院首席法官馬道立日前強調，司法覆核維護公眾利益，...            梁振英\n",
       "18    1643  【港大民調】支持度僅37%創新低　數據顯示董曾梁一蟹不如一蟹 特首梁振英昨日宣讀施政報告，香...            梁振英\n",
       "19    1667  【獨家民調】梁治三年　市民最不滿政制司法　最滿意房屋規劃 今年的施政報告重點力推一帶一路經濟...            梁振英\n",
       "20    1776  奧巴馬炮轟特朗普耍選戰手段　承認未能團結美國 美國總統奧巴馬在電視節目上，批評共和黨總統參選...           美國大選\n",
       "21    1831  【施政報告】預留20億元成立創科創投基金 行政長官梁振英在新一份施政報告提到本港創新科技發展...            梁振英\n",
       "22    2011  由象徵式政策到象徵式施政報告 師父教落，自我介紹中說出來的，通常都不重要，沒有說出來的才重要...            梁振英\n",
       "23    2036  【施政報告】工聯會考慮不支持梁振英　梁連任夢添陰霾 行政長官梁振英發表任內第4份《施政報告》...            梁振英\n",
       "24    2040  港美同日發表施政藍圖　梁振英奧巴馬4大看點 演說長度奧巴馬　58分44秒\\r\\r\\n梁振英 ...            梁振英\n",
       "25    2176  【獨家民調】青年人拒撐梁　無學生支持梁振英連任特首 梁振英昨日（13日）發表《施政報告》後，...            梁振英\n",
       "26    2221  港中戰球迷噓國歌　FIFA再罰香港足總7.7萬 去年「11‧17」港中大戰餘波未了，國際足協...             足球\n",
       "27    2254  【01拆局】曾俊華民望雖高　未獲左派認可　與泛民友好成雙刃劍 特首梁振英新一份施政報告劣評如...            梁振英\n",
       "28    2287  【港足日與夜】從日與夜開始　了解五個香港足球員的故事 經歷去年世界盃外圍賽的熱潮，香港足球再...             足球\n",
       "29    2327  【施政報告】梁振英：即使當年有普選我仍會當選 今年《施政報告》被指「區議會化」，及變成「一帶...            梁振英\n",
       "..     ...                                                ...            ...\n",
       "944  91666  【梁振英UGL案．博評】梁周瓜田李下與議會歪風 梁振英、周浩鼎因UGL調查而鬧得輿論一片沸沸...            梁振英\n",
       "945  91693  【趣聞】女足球員打架誤一生　毆打對手被終身禁賽 有時候一次衝動，足以改變一生。早前波斯尼亞女...             足球\n",
       "946  91699  【01觀點．梁振英UGL案】調查演成爛局　要解釋的不止周浩鼎 若非其身不正　何不大方受查\\r...            梁振英\n",
       "947  91764  【足球明星選舉】費蘭度奪四大獎項　朱志光首嘗最佳教練 11支球隊經過一整季的努力，港超聯決出...             足球\n",
       "948  91774  【拆局】夕陽政府再打郊野公園主意　梁振英搞邊科？ 房協應政府邀請，研究在郊野公園邊陲地帶興建...            梁振英\n",
       "949  91887  【英冠附加賽】高普伴郎率利記借將發功　哈特斯菲爾德晉級鬥雷丁 英超戰局大定，英冠則仍然有一場...             足球\n",
       "950  92044  【拆局】梁振英循「正途」去信委員會　研UGL文件真偽有利案情？ 梁振英今日下午去信調查UGL...            梁振英\n",
       "951  92099  【英超】熱刺大炒李斯特城　卡尼大四喜有望衛冕神射手（有片） 熱刺作客憑哈利卡尼大四喜加上孫興...             足球\n",
       "952  92124  【UGL案】周浩鼎宣布即時辭任委員會委員　重申無違規違法無隱瞞 讓特首梁振英修改交予立法會調...            梁振英\n",
       "953  92165  【港足日與夜．富力3】由中超降格港超　忍受球迷辱罵是成長特訓 R&F富力留在港超踢波被罵得狗...            梁振英\n",
       "954  92413  【德甲】韋比女友來季成德甲主球證　創歐洲主流聯賽歷史（有片） 德甲來季少了拿姆及沙比阿朗素，...             足球\n",
       "955  92455  【港超】南華連續三季四大皆空　錯在球隊停在80年代？ 南華自班主張廣勇入主後，連續3季班費都...             足球\n",
       "956  92462  梁振英送蘭花　證青協總幹事王<U+44EA>鳴退休 青協職員稱不捨 &nbsp; &nbsp...            梁振英\n",
       "957  92494  【UGL案】謝偉俊稱無機制換走梁繼昌　集體決定無人可隻手遮天 行政長官梁振英經民建聯議員周浩...            梁振英\n",
       "958  92506  【德甲】漢堡絕殺反勝禾夫斯堡　連續54季未降班保招牌（有片） 漢堡與禾夫斯堡兩支德甲老牌球隊...             足球\n",
       "959  92584  【港超附加賽】南區3：1慘勝元朗　折多員大將下周決賽撼東方 地區打<U+5421>再次於港超...             足球\n",
       "960  92586  【梁振英UGL案．博評】鼎，請鍾樹根回去做議員吧 【梁振英UGL案】近期熱爆政界的周浩鼎——...            梁振英\n",
       "961  92633  【西甲】皇馬壓巴塞5年首奪西甲　施丹獲贈香檳浴（有片） 5年的等待，終於由施丹達成力壓巴塞隆...             足球\n",
       "962  92706  【英超】阿仙奴成史上最高分第5名　雲格未定下季去留 雲格執教阿仙奴20年，首次未能帶領球隊獲...             足球\n",
       "963  92742  【港超】盧卡斯率先轉會傑志　有望周五鬥熱刺 今季為和富大埔贏得菁英盃的功臣盧卡斯，一直盛傳會...             足球\n",
       "964  92744  【UGL案】梁振英繼續狙擊梁繼昌　指無回應質疑「道理何在」？ 梁繼昌在多名民主派議員陪同下召...            梁振英\n",
       "965  92770  【英超】泰利盧卡斯薩巴列達說再見　別了熟悉的面孔（有片） 一代新人換舊人，下一季英超，追捧多...             足球\n",
       "966  92817  梁振英罕有推小冊子晒政績　尹兆堅：他想幫自己政治上風光大葬 本屆政府臨近尾聲，行政長官梁振英...            梁振英\n",
       "967  92990  【熱刺訪港】卡尼阿里簽名不停手　普捷天奴揚言下季爭英超 應屆英超亞軍熱刺周二抵港，準備周五於...             足球\n",
       "968  93063  【政圈風聲】UGL餘波愈演愈烈　林鄭或淪另類受害者？ 「浩鼎門」一石激起千重浪，梁振英連日與...            梁振英\n",
       "969  93507  【熱刺訪港】普捷天奴成搶手貨　主席利維開腔派定心丸 英超今季群雄割據，摩連奴、干地及哥迪奧拿...             足球\n",
       "970  93651  【熱刺訪港】孫興<U+615C>林志堅再聚舊　承諾賽後交換球衣 熱刺周五將與傑志於香港大球場...             足球\n",
       "971  93690  【港足日與夜．王振鵬】膠唔會膠一世　神經刀變神龍（有片） 有些球員出道十多年，一直都被人睇低...             足球\n",
       "972  93985  【中超】泰維斯抱怨遭中超球員踢傷　澄清無意離開上海申花 受人錢財不一定替人消災，阿根廷前鋒泰...             足球\n",
       "973  94324  【傑志對熱刺．來稿】睇波睇到開party咁先至過癮 剛過去的周末，香港刮起一股足球熱。先是上...             足球\n",
       "\n",
       "[974 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('../output/testset_with_tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
