{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use hyperas for hyperparam tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import polyglot\n",
    "from polyglot.text import Text, Word\n",
    "from polyglot.mapping import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense, LSTM, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    MAX_WORD_COUNT = 150\n",
    "    class TextClassificationDataSet(object):\n",
    "        def __init__(self, \n",
    "                     file_path,\n",
    "                     word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
    "                     MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
    "                     text_col_name='text',\n",
    "                     label_col_name='tags',\n",
    "                     one_hot_encoder=None):\n",
    "            self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
    "\n",
    "            self.df = pd.read_csv(file_path)\n",
    "            self.text_col_name = text_col_name\n",
    "            self.label_col_name = label_col_name\n",
    "\n",
    "            if label_col_name is not None:\n",
    "                self.label_encoder = self._fit_label_encoder(label_col_name)\n",
    "                self.onehot_encoder = self._fit_onehot_encoder()\n",
    "\n",
    "            if one_hot_encoder is not None:\n",
    "                self.one_hot_encoder = one_hot_encoder\n",
    "\n",
    "            self.embeddings = self._load_word_embeddings(word_embedding)\n",
    "\n",
    "            self.features = None\n",
    "            self.labels = None\n",
    "\n",
    "        def get_features(self, use_cache=True):        \n",
    "            if use_cache and self.features is not None:\n",
    "                return self.features\n",
    "            clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
    "            self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
    "            return self.features\n",
    "\n",
    "        def get_labels(self, use_cache=True):\n",
    "            if self.label_col_name is None:\n",
    "                raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
    "            if use_cache and self.labels is not None:\n",
    "                return self.labels\n",
    "            self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
    "            return self.labels\n",
    "\n",
    "        def _parse_text(self, text):\n",
    "            if isinstance(text, str):\n",
    "                text_parsed = Text(text)\n",
    "            else:\n",
    "                text_parsed = text\n",
    "            return text_parsed\n",
    "\n",
    "        def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
    "            if isinstance(article_text, str):\n",
    "                article_parsed = self._parse_text(article_text)\n",
    "\n",
    "            sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
    "            return sentences_words_embedding\n",
    "\n",
    "        def _load_word_embeddings(self, word_embedding):\n",
    "            if isinstance(word_embedding, Embedding):\n",
    "                return word_embedding\n",
    "            else:\n",
    "                return Embedding.load(word_embedding)\n",
    "\n",
    "        def _load_data_from_csv(self, file_path):\n",
    "            return pd.read_csv(file_path)\n",
    "\n",
    "        def _get_clean_text_col(self, text_col):\n",
    "            \"\"\"remove html tags in text\"\"\"\n",
    "            text_col = self.df[text_col]\n",
    "            return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
    "\n",
    "        def _fit_label_encoder(self, label_col):\n",
    "            label_encoder = preprocessing.LabelEncoder()\n",
    "            label_encoder.fit(self.df[label_col].tolist())\n",
    "            self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
    "            self.label_encoder = label_encoder\n",
    "            return label_encoder\n",
    "\n",
    "        def _fit_onehot_encoder(self):\n",
    "            onehot_encoder = preprocessing.OneHotEncoder()\n",
    "            onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
    "            self.onehot_encoder = onehot_encoder\n",
    "            return onehot_encoder\n",
    "    dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(dataset_train.get_features(), dataset_train.get_labels(), test_size=0.2, random_state=42)\n",
    "    return X_train, y_train, X_validate, y_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_first500():\n",
    "    MAX_WORD_COUNT = 150\n",
    "    first_n_records = 500\n",
    "    class TextClassificationDataSet(object):\n",
    "        def __init__(self, \n",
    "                     file_path,\n",
    "                     word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
    "                     MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
    "                     text_col_name='text',\n",
    "                     label_col_name='tags',\n",
    "                     one_hot_encoder=None):\n",
    "            self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
    "\n",
    "            self.df = pd.read_csv(file_path).head(first_n_records)\n",
    "            self.text_col_name = text_col_name\n",
    "            self.label_col_name = label_col_name\n",
    "\n",
    "            if label_col_name is not None:\n",
    "                self.label_encoder = self._fit_label_encoder(label_col_name)\n",
    "                self.onehot_encoder = self._fit_onehot_encoder()\n",
    "\n",
    "            if one_hot_encoder is not None:\n",
    "                self.one_hot_encoder = one_hot_encoder\n",
    "\n",
    "            self.embeddings = self._load_word_embeddings(word_embedding)\n",
    "\n",
    "            self.features = None\n",
    "            self.labels = None\n",
    "\n",
    "        def get_features(self, use_cache=True):        \n",
    "            if use_cache and self.features is not None:\n",
    "                return self.features\n",
    "            clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
    "            self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
    "            return self.features\n",
    "\n",
    "        def get_labels(self, use_cache=True):\n",
    "            if self.label_col_name is None:\n",
    "                raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
    "            if use_cache and self.labels is not None:\n",
    "                return self.labels\n",
    "            self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
    "            return self.labels\n",
    "\n",
    "        def _parse_text(self, text):\n",
    "            if isinstance(text, str):\n",
    "                text_parsed = Text(text)\n",
    "            else:\n",
    "                text_parsed = text\n",
    "            return text_parsed\n",
    "\n",
    "        def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
    "            if isinstance(article_text, str):\n",
    "                article_parsed = self._parse_text(article_text)\n",
    "\n",
    "            sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
    "            return sentences_words_embedding\n",
    "\n",
    "        def _load_word_embeddings(self, word_embedding):\n",
    "            if isinstance(word_embedding, Embedding):\n",
    "                return word_embedding\n",
    "            else:\n",
    "                return Embedding.load(word_embedding)\n",
    "\n",
    "        def _load_data_from_csv(self, file_path):\n",
    "            return pd.read_csv(file_path)\n",
    "\n",
    "        def _get_clean_text_col(self, text_col):\n",
    "            \"\"\"remove html tags in text\"\"\"\n",
    "            text_col = self.df[text_col]\n",
    "            return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
    "\n",
    "        def _fit_label_encoder(self, label_col):\n",
    "            label_encoder = preprocessing.LabelEncoder()\n",
    "            label_encoder.fit(self.df[label_col].tolist())\n",
    "            self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
    "            self.label_encoder = label_encoder\n",
    "            return label_encoder\n",
    "\n",
    "        def _fit_onehot_encoder(self):\n",
    "            onehot_encoder = preprocessing.OneHotEncoder()\n",
    "            onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
    "            self.onehot_encoder = onehot_encoder\n",
    "            return onehot_encoder\n",
    "    dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(dataset_train.get_features(), dataset_train.get_labels(), test_size=0.2, random_state=42)\n",
    "    return X_train, y_train, X_validate, y_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_model_seq(X_train, y_train, X_validate, y_validate):\n",
    "    MAX_WORD_COUNT = 150\n",
    "    embedding_size = 64\n",
    "    tag_classes_count = 3\n",
    "    \n",
    "    batch_size = {{choice([128, 256, 512])}}\n",
    "    lstm_units = {{choice([64, 128, 256, 512])}}\n",
    "    dense_units = {{choice([64, 128, 256, 512])}}\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(lstm_units, input_shape=(MAX_WORD_COUNT, embedding_size), name='LSTM'))\n",
    "    \n",
    "    model.add(Dense(dense_units, activation='relu', name='Dense_1'))\n",
    "    model.add(Dense(dense_units, activation='relu', name='Dense_2'))\n",
    "    model.add(Dense(dense_units, activation='relu', name='Dense_3'))\n",
    "\n",
    "    model.add(Dense(tag_classes_count, activation='softmax', name='main_output'))\n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam', 'adagrad', 'nadam', 'adadelta'])}}, \n",
    "              loss={'main_output': 'categorical_crossentropy'}, \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              validation_data=(X_validate, y_validate))\n",
    "    \n",
    "    score, acc = model.evaluate(X_validate, y_validate, batch_size=batch_size, verbose=0)\n",
    "    print('Test Accuracy:{}'.format(acc))\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_validate, y_validate = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from bs4 import BeautifulSoup\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import polyglot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from polyglot.text import Text, Word\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from polyglot.mapping import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, LSTM, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model, Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import optimizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from bs4 import BeautifulSoup\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn import preprocessing\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import polyglot\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from polyglot.text import Text, Word\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from polyglot.mapping import Embedding\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import sequence\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import accuracy_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.backend import constant\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Dense, LSTM, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model, Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import optimizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe, rand\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import gc\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'batch_size': hp.choice('batch_size', [128, 256, 512]),\n",
      "        'lstm_units': hp.choice('lstm_units', [64, 128, 256, 512]),\n",
      "        'lstm_units_1': hp.choice('lstm_units_1', [64, 128, 256, 512]),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'adagrad', 'nadam', 'adadelta']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: MAX_WORD_COUNT = 150\n",
      "   3: first_n_records = 500\n",
      "   4: class TextClassificationDataSet(object):\n",
      "   5:     def __init__(self, \n",
      "   6:                  file_path,\n",
      "   7:                  word_embedding='./polyglot/embeddings2/zh/embeddings_pkl.tar.bz2',\n",
      "   8:                  MAX_WORD_COUNT=MAX_WORD_COUNT,\n",
      "   9:                  text_col_name='text',\n",
      "  10:                  label_col_name='tags',\n",
      "  11:                  one_hot_encoder=None):\n",
      "  12:         self.MAX_WORD_COUNT = MAX_WORD_COUNT\n",
      "  13: \n",
      "  14:         self.df = pd.read_csv(file_path).head(first_n_records)\n",
      "  15:         self.text_col_name = text_col_name\n",
      "  16:         self.label_col_name = label_col_name\n",
      "  17: \n",
      "  18:         if label_col_name is not None:\n",
      "  19:             self.label_encoder = self._fit_label_encoder(label_col_name)\n",
      "  20:             self.onehot_encoder = self._fit_onehot_encoder()\n",
      "  21: \n",
      "  22:         if one_hot_encoder is not None:\n",
      "  23:             self.one_hot_encoder = one_hot_encoder\n",
      "  24: \n",
      "  25:         self.embeddings = self._load_word_embeddings(word_embedding)\n",
      "  26: \n",
      "  27:         self.features = None\n",
      "  28:         self.labels = None\n",
      "  29: \n",
      "  30:     def get_features(self, use_cache=True):        \n",
      "  31:         if use_cache and self.features is not None:\n",
      "  32:             return self.features\n",
      "  33:         clean_text_col = self._get_clean_text_col(self.text_col_name)\n",
      "  34:         self.features = np.array(clean_text_col.apply(lambda x: np.squeeze(self._article2vecs_simple(x, embeddings=self.embeddings, max_word_count=self.MAX_WORD_COUNT))).tolist())\n",
      "  35:         return self.features\n",
      "  36: \n",
      "  37:     def get_labels(self, use_cache=True):\n",
      "  38:         if self.label_col_name is None:\n",
      "  39:             raise KeyError('label_col_name is None, unable to get labels from the input data.')\n",
      "  40:         if use_cache and self.labels is not None:\n",
      "  41:             return self.labels\n",
      "  42:         self.labels = self.onehot_encoder.transform(self.df['label_index'].values.reshape(-1, 1)).toarray()\n",
      "  43:         return self.labels\n",
      "  44: \n",
      "  45:     def _parse_text(self, text):\n",
      "  46:         if isinstance(text, str):\n",
      "  47:             text_parsed = Text(text)\n",
      "  48:         else:\n",
      "  49:             text_parsed = text\n",
      "  50:         return text_parsed\n",
      "  51: \n",
      "  52:     def _article2vecs_simple(self, article_text, embeddings, max_word_count):\n",
      "  53:         if isinstance(article_text, str):\n",
      "  54:             article_parsed = self._parse_text(article_text)\n",
      "  55: \n",
      "  56:         sentences_words_embedding = sequence.pad_sequences([[embeddings.get(word) for word in article_parsed.words if embeddings.get(word) is not None]], maxlen=max_word_count, truncating='post', dtype='float32')\n",
      "  57:         return sentences_words_embedding\n",
      "  58: \n",
      "  59:     def _load_word_embeddings(self, word_embedding):\n",
      "  60:         if isinstance(word_embedding, Embedding):\n",
      "  61:             return word_embedding\n",
      "  62:         else:\n",
      "  63:             return Embedding.load(word_embedding)\n",
      "  64: \n",
      "  65:     def _load_data_from_csv(self, file_path):\n",
      "  66:         return pd.read_csv(file_path)\n",
      "  67: \n",
      "  68:     def _get_clean_text_col(self, text_col):\n",
      "  69:         \"\"\"remove html tags in text\"\"\"\n",
      "  70:         text_col = self.df[text_col]\n",
      "  71:         return text_col.apply(lambda x: BeautifulSoup(x, \"html5lib\").text)\n",
      "  72: \n",
      "  73:     def _fit_label_encoder(self, label_col):\n",
      "  74:         label_encoder = preprocessing.LabelEncoder()\n",
      "  75:         label_encoder.fit(self.df[label_col].tolist())\n",
      "  76:         self.df['label_index'] = label_encoder.fit_transform(self.df[label_col])\n",
      "  77:         self.label_encoder = label_encoder\n",
      "  78:         return label_encoder\n",
      "  79: \n",
      "  80:     def _fit_onehot_encoder(self):\n",
      "  81:         onehot_encoder = preprocessing.OneHotEncoder()\n",
      "  82:         onehot_encoder.fit(self.df['label_index'].values.reshape(-1, 1))\n",
      "  83:         self.onehot_encoder = onehot_encoder\n",
      "  84:         return onehot_encoder\n",
      "  85: dataset_train = TextClassificationDataSet(file_path='../data/offsite-tagging-training-set (1).csv')\n",
      "  86: X_train, X_validate, y_train, y_validate = train_test_split(dataset_train.get_features(), dataset_train.get_labels(), test_size=0.2, random_state=42)\n",
      "  87: \n",
      "  88: \n",
      "  89: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     MAX_WORD_COUNT = 150\n",
      "   4:     embedding_size = 64\n",
      "   5:     tag_classes_count = 3\n",
      "   6:     \n",
      "   7:     batch_size = space['batch_size']\n",
      "   8:     lstm_units = space['lstm_units']\n",
      "   9:     dense_units = space['lstm_units_1']\n",
      "  10:     \n",
      "  11:     model = Sequential()\n",
      "  12: \n",
      "  13:     model.add(LSTM(lstm_units, input_shape=(MAX_WORD_COUNT, embedding_size), name='LSTM'))\n",
      "  14:     \n",
      "  15:     model.add(Dense(dense_units, activation='relu', name='Dense_1'))\n",
      "  16:     model.add(Dense(dense_units, activation='relu', name='Dense_2'))\n",
      "  17:     model.add(Dense(dense_units, activation='relu', name='Dense_3'))\n",
      "  18: \n",
      "  19:     model.add(Dense(tag_classes_count, activation='softmax', name='main_output'))\n",
      "  20:     model.compile(optimizer=space['optimizer'], \n",
      "  21:               loss={'main_output': 'categorical_crossentropy'}, \n",
      "  22:               metrics=['accuracy'])\n",
      "  23:     \n",
      "  24:     model.fit(X_train, y_train,\n",
      "  25:               batch_size=batch_size,\n",
      "  26:               epochs=1,\n",
      "  27:               validation_data=(X_validate, y_validate))\n",
      "  28:     \n",
      "  29:     score, acc = model.evaluate(X_validate, y_validate, batch_size=batch_size, verbose=0)\n",
      "  30:     print('Test Accuracy:{}'.format(acc))\n",
      "  31:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  32: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 10s - loss: 1.0799 - acc: 0.4200 - val_loss: 0.9356 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 5s - loss: 1.0695 - acc: 0.5200 - val_loss: 0.9555 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 2s - loss: 1.1968 - acc: 0.3675 - val_loss: 1.0827 - val_acc: 0.6400\n",
      "Test Accuracy:0.6399999856948853\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 20s - loss: 1.0580 - acc: 0.4325 - val_loss: 0.8704 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 23s - loss: 1.1058 - acc: 0.2350 - val_loss: 0.9998 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 8s - loss: 1.0989 - acc: 0.3525 - val_loss: 5.4310 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 3s - loss: 1.0346 - acc: 0.5250 - val_loss: 0.8687 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 19s - loss: 2.6716 - acc: 0.3325 - val_loss: 1.0771 - val_acc: 0.2900\n",
      "Test Accuracy:0.28999999165534973\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 20s - loss: 1.0966 - acc: 0.3875 - val_loss: 1.0561 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 20s - loss: 1.1941 - acc: 0.4050 - val_loss: 1.0869 - val_acc: 0.6000\n",
      "Test Accuracy:0.6000000238418579\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 22s - loss: 1.1005 - acc: 0.2925 - val_loss: 0.9627 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 2s - loss: 1.1169 - acc: 0.2225 - val_loss: 1.0887 - val_acc: 0.3100\n",
      "Test Accuracy:0.3100000023841858\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 21s - loss: 1.1049 - acc: 0.2425 - val_loss: 0.9689 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 3s - loss: 1.0618 - acc: 0.5175 - val_loss: 0.8591 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 3s - loss: 1.0582 - acc: 0.5250 - val_loss: 0.8809 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 4s - loss: 1.0875 - acc: 0.4625 - val_loss: 1.0347 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 5s - loss: 1.0802 - acc: 0.4925 - val_loss: 0.9664 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 20s - loss: 1.0700 - acc: 0.3975 - val_loss: 0.9974 - val_acc: 0.6500\n",
      "Test Accuracy:0.6499999761581421\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 3s - loss: 1.0976 - acc: 0.2900 - val_loss: 1.0604 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 4s - loss: 1.0598 - acc: 0.5150 - val_loss: 0.9062 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 8s - loss: 1.0903 - acc: 0.3900 - val_loss: 1.0166 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 10s - loss: 1.0978 - acc: 0.3750 - val_loss: 1.0751 - val_acc: 0.6500\n",
      "Test Accuracy:0.6499999761581421\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 11s - loss: 1.0570 - acc: 0.4050 - val_loss: 0.8238 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 8s - loss: 1.0861 - acc: 0.4650 - val_loss: 1.0008 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 9s - loss: 1.0669 - acc: 0.3950 - val_loss: 0.8709 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 9s - loss: 1.0790 - acc: 0.4575 - val_loss: 0.9427 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 9s - loss: 1.1031 - acc: 0.3275 - val_loss: 1.0741 - val_acc: 0.6700\n",
      "Test Accuracy:0.6700000166893005\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 9s - loss: 1.1087 - acc: 0.3125 - val_loss: 0.8860 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 5s - loss: 1.0839 - acc: 0.4700 - val_loss: 1.0231 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 9s - loss: 1.5698 - acc: 0.4550 - val_loss: 1.1123 - val_acc: 0.1600\n",
      "Test Accuracy:0.1599999964237213\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 5s - loss: 1.1085 - acc: 0.2275 - val_loss: 1.0883 - val_acc: 0.4200\n",
      "Test Accuracy:0.41999998688697815\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 9s - loss: 1.0797 - acc: 0.4650 - val_loss: 0.9663 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 13s - loss: 1.3891 - acc: 0.4100 - val_loss: 1.0845 - val_acc: 0.5900\n",
      "Test Accuracy:0.5899999737739563\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 11s - loss: 1.0971 - acc: 0.3550 - val_loss: 0.9994 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 6s - loss: 1.0776 - acc: 0.3725 - val_loss: 0.8539 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 10s - loss: 1.7359 - acc: 0.5200 - val_loss: 1.0855 - val_acc: 0.5000\n",
      "Test Accuracy:0.5\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 11s - loss: 1.0899 - acc: 0.3950 - val_loss: 0.9716 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 10s - loss: 1.0801 - acc: 0.5250 - val_loss: 1.0140 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 7s - loss: 1.0850 - acc: 0.4075 - val_loss: 0.8904 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 10s - loss: 1.0700 - acc: 0.5200 - val_loss: 0.9613 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 12s - loss: 1.1008 - acc: 0.2475 - val_loss: 1.0534 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 22s - loss: 3.7230 - acc: 0.4175 - val_loss: 5.4802 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 9s - loss: 1.1074 - acc: 0.3200 - val_loss: 1.0405 - val_acc: 0.6500\n",
      "Test Accuracy:0.6499999761581421\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 6s - loss: 1.0892 - acc: 0.5100 - val_loss: 0.9759 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 22s - loss: 1.2587 - acc: 0.3450 - val_loss: 1.0765 - val_acc: 0.6500\n",
      "Test Accuracy:0.6499999761581421\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 11s - loss: 1.0783 - acc: 0.4375 - val_loss: 0.9133 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 5s - loss: 1.1050 - acc: 0.2700 - val_loss: 1.0866 - val_acc: 0.5200\n",
      "Test Accuracy:0.5199999809265137\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 30s - loss: 1.0970 - acc: 0.3600 - val_loss: 0.9945 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 10s - loss: 3.4009 - acc: 0.3575 - val_loss: 1.2270 - val_acc: 0.1200\n",
      "Test Accuracy:0.11999999731779099\n",
      "Train on 400 samples, validate on 100 samples\n",
      "Epoch 1/1\n",
      "400/400 [==============================] - 5s - loss: 1.0981 - acc: 0.2725 - val_loss: 1.0663 - val_acc: 0.6600\n",
      "Test Accuracy:0.6600000262260437\n"
     ]
    }
   ],
   "source": [
    "# import gc; gc.collect()\n",
    "trials = Trials()\n",
    "best_run, best_model, space = optim.minimize(model=search_model_seq,\n",
    "                                      data=data_first500,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=50,\n",
    "                                      trials=trials,\n",
    "                                      notebook_name='draft',\n",
    "                                             eval_space=True,   # <-- this is the line that puts real values into 'best_run'\n",
    "                                             return_space=True  # <-- this allows you to save the space for later evaluations \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "779/779 [==============================] - 5s     \n",
      "[1.0598476840534627, 0.5430038513206854]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'batch_size': 256, 'lstm_units': 256, 'lstm_units_1': 64, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_validate, y_validate))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 256, 'lstm_units': 256, 'lstm_units_1': 64, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3115 samples, validate on 779 samples\n",
      "Epoch 1/10\n",
      "3115/3115 [==============================] - 62s - loss: 0.9635 - acc: 0.5461 - val_loss: 0.8307 - val_acc: 0.5738\n",
      "Epoch 2/10\n",
      "3115/3115 [==============================] - 56s - loss: 0.7942 - acc: 0.6575 - val_loss: 0.6543 - val_acc: 0.7304\n",
      "Epoch 3/10\n",
      "3115/3115 [==============================] - 57s - loss: 0.5758 - acc: 0.7589 - val_loss: 0.5673 - val_acc: 0.7099\n",
      "Epoch 4/10\n",
      "3115/3115 [==============================] - 55s - loss: 0.5295 - acc: 0.8112 - val_loss: 0.4753 - val_acc: 0.8357\n",
      "Epoch 5/10\n",
      "3115/3115 [==============================] - 63s - loss: 0.4303 - acc: 0.8607 - val_loss: 0.2549 - val_acc: 0.9166\n",
      "Epoch 6/10\n",
      "3115/3115 [==============================] - 61s - loss: 0.3139 - acc: 0.8941 - val_loss: 0.2204 - val_acc: 0.9204\n",
      "Epoch 7/10\n",
      "3115/3115 [==============================] - 59s - loss: 0.2678 - acc: 0.9053 - val_loss: 0.2284 - val_acc: 0.9294\n",
      "Epoch 8/10\n",
      "3115/3115 [==============================] - 57s - loss: 0.2923 - acc: 0.8970 - val_loss: 0.2689 - val_acc: 0.9089\n",
      "Epoch 9/10\n",
      "3115/3115 [==============================] - 59s - loss: 0.2521 - acc: 0.9159 - val_loss: 0.2007 - val_acc: 0.9307\n",
      "Epoch 10/10\n",
      "3115/3115 [==============================] - 57s - loss: 0.2405 - acc: 0.9172 - val_loss: 0.3198 - val_acc: 0.9037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127dab198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(X_train, y_train,\n",
    "              batch_size=best_run['batch_size'],\n",
    "              epochs=10,\n",
    "#               verbose=2,\n",
    "              validation_data=(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779/779 [==============================] - 6s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31981858569024002, 0.90372272174379797]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
